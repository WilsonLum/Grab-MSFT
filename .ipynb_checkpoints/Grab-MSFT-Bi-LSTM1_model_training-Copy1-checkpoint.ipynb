{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab-Microsoft Challenge \n",
    "## Traffic Management\n",
    "\n",
    "- geohash6: geohash is a public domain geocoding system which encodes a geographic location into a short string of letters and digits with arbitrary precision. You are free to use any geohash library to encode/decode the geohashes into latitude and longitude or vice versa.(Examples:https://github.com/hkwi/python-geohash)\n",
    "- day: the value indicates the sequential order and not a particular day of the month\n",
    "- timestamp: start time of 15-minute intervals in the following format: <hour>:<minute>, where hour ranges from 0 to 23 and minute is either one of (0, 15, 30, 45)\n",
    "- demand: aggregated demand normalised to be in the range [0,1]\n",
    "    \n",
    "## Problem Statements:\n",
    "- Which areas have high / low traffic demand?\n",
    "- How does regional traffic demand change according to day / time?\n",
    "- Forecast the travel demand for next 15min / 1hour and predict areas with high travel demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is train LSTM model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot matplotlib graph\n",
    "%matplotlib inline\n",
    "\n",
    "#Import models from scikit learn module:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, LSTM, BatchNormalization\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import h5py\n",
    "import sklearn.metrics as metrics\n",
    "from keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.utils import plot_model \n",
    "\n",
    "import pickle\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "modelname   = 'Bi-LSTM1'\n",
    "batch_size  = 8192\n",
    "no_of_epoch = 18\n",
    "no_of_train = 0.8\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, predict_next_no_of_output = 3, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(): \n",
    "    inputs  = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
    "    y = Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(inputs)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dense(predict_next_no_of_output, activation='sigmoid')(y)\n",
    "  \n",
    "    model = Model(inputs=inputs,outputs=y)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "def createModel(): \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(20, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(predict_next_no_of_output, activation='sigmoid')(y))\n",
    "\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['mse', 'mae'])\n",
    "    return model\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading from Preprocessed & Test sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xls  = pd.ExcelFile('data/test_sample.xlsx')\n",
    "test_sample = pd.read_excel(xls, 'TrafficMgmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls  = pd.ExcelFile('data/Dataset_feature.xlsx')\n",
    "data = pd.read_excel(xls, 'TrafficMgmt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['day', 'hour', 'min', 'dow', 'lat', 'long']\n",
    "outcome_var   = 'demand'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training & test data\n",
    "- Since this is a time series dataset, we can try using LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, predict_next_no_of_output)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test-val dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X[:int(no_of_train*len(X))]\n",
    "y_train1 = y[:int(no_of_train*len(X))]\n",
    "X_test  = X[int(no_of_train*len(X)):]\n",
    "y_test  = y[int(no_of_train*len(X)):]\n",
    "\n",
    "X_train = X_train1[:int(no_of_train*len(X_train1))]\n",
    "y_train = y_train1[:int(no_of_train*len(X_train1))]\n",
    "X_val   = X_train1[int(no_of_train*len(X_train1)):]\n",
    "y_val   = y_train1[int(no_of_train*len(X_train1)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape,X_test.shape, y_test.shape,X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Create Model and test</center><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LSTM<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel1()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',optimizer='adam', metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoints to save model during training and save training data into csv\n",
    "# ‘monitor’ can be ‘val_acc’ or ‘val_loss’\n",
    "# When set to ‘val_acc’, ‘mode’ must be ‘max’; when set to ‘val_loss’, ‘mode’ must be ‘min’\n",
    "\n",
    "filepath       = modelname + \".hdf5\"\n",
    "checkpoint     = ModelCheckpoint(filepath, monitor='val_loss',verbose=0,save_best_only=True,mode='min') \n",
    "csv_logger     = CSVLogger(modelname + '.csv')\n",
    "callbacks_list = [checkpoint,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line for training\n",
    "hisgtory = model.fit(X_train, \n",
    "                     y_train, \n",
    "                     validation_data=(X_val, y_val), \n",
    "                     epochs=no_of_epoch, \n",
    "                     batch_size=batch_size,\n",
    "                     shuffle=False,\n",
    "                     callbacks=callbacks_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, \n",
    "             to_file=modelname + '.pdf', \n",
    "             show_shapes=True, \n",
    "             show_layer_names=False,\n",
    "             rankdir='TB') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation dataset test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(X_val)\n",
    "print(mean_squared_error(y_val,predicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value = model.predict(X_test)\n",
    "print(mean_squared_error(y_test,predicted_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records     = pd.read_csv(modelname +'.csv')\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(records['val_mse'],label=\"val_mse\")\n",
    "plt.plot(records['val_loss'],label=\"val_loss\")\n",
    "plt.plot(records['loss'],label=\"loss\")\n",
    "plt.title('MSE',fontsize=12)\n",
    "plt.legend(loc=\"upper left\",fontsize=15)\n",
    "\n",
    "ax          = plt.gca()\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(records['val_mae'],label=\"val_mae\")\n",
    "plt.plot(records['val_loss'],label=\"val_loss\")\n",
    "plt.plot(records['loss'],label=\"loss\")\n",
    "plt.title('MAE',fontsize=12)\n",
    "plt.legend(loc=\"upper left\",fontsize=15)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load save LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your own trained model\n",
    "model = load_model(filepath, compile = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"grab_msft_scaler.save\"\n",
    "scaler          = joblib.load(scaler_filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First set of testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Standardscalar to sample test data\n",
    "test_sample[predictor_var] = scaler.fit_transform(test_sample[predictor_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert into input/output\n",
    "test_sample_array = test_sample.to_numpy()\n",
    "X_sample, y_sample = split_sequences(test_sample_array, n_steps_in, predict_next_no_of_output)\n",
    "print(X_sample.shape, y_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value = model.predict(X_sample)\n",
    "print('Predicted value : {} \\n Actual Value    : {}' .format(predicted_value,y_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(y_sample,predicted_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data input sequence format :\n",
    "test_data1  = [[18,20, 0, 4, -5.353088, 90.653687],\n",
    "               [10,14,30, 3, -5.413513, 90.664673],\n",
    "               [ 9, 6,15, 2, -5.325623, 90.906372]]\n",
    "test_data2  = [[32, 5, 0, 4, -5.353088, 90.752563],\n",
    "               [15, 4, 0, 1, -5.413513, 90.719604],\n",
    "               [ 1,12,15, 1, -5.336609, 90.609741]]\n",
    "test_data3  = [[25, 3,30, 4, -5.391541, 90.818481],\n",
    "               [51,20,45, 2, -5.408020, 90.631714],\n",
    "               [48, 6,15, 6, -5.364075, 90.763550]]\n",
    "test_data4  = [[ 4,22,15, 4, -5.402527, 90.675659],\n",
    "               [45, 9,15, 3, -5.402527, 90.917358],\n",
    "               [52,11,45, 3, -5.364075, 90.664673]]\n",
    "test_data5  = [[46,12,15, 4, -5.353088, 90.642700],\n",
    "               [34,14,45, 6, -5.375061, 90.807495],\n",
    "               [40, 2,30, 5, -5.424500, 90.785522]]\n",
    "test_data6  = [[14,14,45, 0, -5.391541, 90.598755],\n",
    "               [27, 3,30, 6, -5.320129, 90.785522],\n",
    "               [ 6,23,45, 6, -5.358582, 90.752563]]\n",
    "test_data7  = [[48,11,30, 6, -5.391541, 90.609741],\n",
    "                [17,23,45, 3, -5.292664, 90.829468],\n",
    "                [56,10, 0, 0, -5.413513, 90.774536]]\n",
    "\n",
    "test_target1 = [[0.102821],[0.088755]]\n",
    "test_target2 = [[0.023843],[0.007460]]\n",
    "test_target3 = [[0.054170],[0.123463]]\n",
    "test_target4 = [[0.359406],[0.514136]]\n",
    "test_target5 = [[0.026409],[0.013998]]\n",
    "test_target6 = [[0.029400],[0.057255]]\n",
    "test_target7 = [[0.008772],[0.119240]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scaled = scaler.transform(test_data1)\n",
    "Data_scaled = Data_scaled.reshape(1,n_steps_in,6)\n",
    "predicted_value = model.predict(Data_scaled)\n",
    "print('Predicted value : {}\\nActual Value    : {}' .format(predicted_value,test_target1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scaled = scaler.transform(test_data2)\n",
    "Data_scaled = Data_scaled.reshape(1,n_steps_in,6)\n",
    "predicted_value = model.predict(Data_scaled)\n",
    "print('Predicted value : {}\\nActual Value    : {}' .format(predicted_value,test_target2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scaled = scaler.transform(test_data3)\n",
    "Data_scaled = Data_scaled.reshape(1,n_steps_in,6)\n",
    "predicted_value = model.predict(Data_scaled)\n",
    "print('Predicted value : {}\\nActual Value    : {}' .format(predicted_value,test_target3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scaled = scaler.transform(test_data4)\n",
    "Data_scaled = Data_scaled.reshape(1,n_steps_in,6)\n",
    "predicted_value = model.predict(Data_scaled)\n",
    "print('Predicted value : {}\\nActual Value    : {}' .format(predicted_value,test_target4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scaled = scaler.transform(test_data5)\n",
    "Data_scaled = Data_scaled.reshape(1,n_steps_in,6)\n",
    "predicted_value = model.predict(Data_scaled)\n",
    "print('Predicted value : {}\\nActual Value    : {}' .format(predicted_value,test_target5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scaled = scaler.transform(test_data6)\n",
    "Data_scaled = Data_scaled.reshape(1,n_steps_in,6)\n",
    "predicted_value = model.predict(Data_scaled)\n",
    "print('Predicted value : {}\\nActual Value    : {}' .format(predicted_value,test_target6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scaled = scaler.transform(test_data7)\n",
    "Data_scaled = Data_scaled.reshape(1,n_steps_in,6)\n",
    "predicted_value = model.predict(Data_scaled)\n",
    "print('Predicted value : {}\\nActual Value    : {}' .format(predicted_value,test_target7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
