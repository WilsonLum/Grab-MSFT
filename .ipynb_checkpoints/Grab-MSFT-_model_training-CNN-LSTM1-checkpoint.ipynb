{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab-Microsoft Challenge \n",
    "## Traffic Management\n",
    "\n",
    "- geohash6: geohash is a public domain geocoding system which encodes a geographic location into a short string of letters and digits with arbitrary precision. You are free to use any geohash library to encode/decode the geohashes into latitude and longitude or vice versa.(Examples:https://github.com/hkwi/python-geohash)\n",
    "- day: the value indicates the sequential order and not a particular day of the month\n",
    "- timestamp: start time of 15-minute intervals in the following format: <hour>:<minute>, where hour ranges from 0 to 23 and minute is either one of (0, 15, 30, 45)\n",
    "- demand: aggregated demand normalised to be in the range [0,1]\n",
    "    \n",
    "## Problem Statements:\n",
    "- Which areas have high / low traffic demand?\n",
    "- How does regional traffic demand change according to day / time?\n",
    "- Forecast the travel demand for next 15min / 1hour and predict areas with high travel demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is for model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot matplotlib graph\n",
    "%matplotlib inline\n",
    "\n",
    "#Import models from scikit learn module:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import h5py\n",
    "import sklearn.metrics as metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from ModelDefinitions import createModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #***********************************\n",
    "    # Index 00-10 is for LSTM Model\n",
    "    # Index 11-20 is for Bi-LSTM Model\n",
    "    # Index 21-30 is for CNN Model\n",
    "    # Index 31-40 is for CNN-LSTM Model\n",
    "    #***********************************\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, predict_next_no_of_output = 3, 2\n",
    "\n",
    "# ------ CHANGE THESE ------\n",
    "index = 32\n",
    "seed  = 7\n",
    "np.random.seed(seed)\n",
    "basemodelname = 'CNN-LSTM2-' + str(n_steps_in) + 'in' + str(predict_next_no_of_output) + 'out'\n",
    "batch_size    = 4096\n",
    "no_of_epoch   = 30\n",
    "no_of_train   = 0.8\n",
    "# --------------------------\n",
    "\n",
    "modelname = 'model/' + basemodelname + \"_\" + str(index)\n",
    "filepath  = modelname + \".hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls  = pd.ExcelFile('data/Dataset_Sorted_feature.xlsx')\n",
    "data = pd.read_excel(xls, 'TrafficMgmt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['day','hour','min','dow','lat','long']\n",
    "outcome_var   = 'demand'\n",
    "no_of_features = len(predictor_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into input/output\n",
    "dataset = data.to_numpy()\n",
    "X, y = split_sequences(dataset, n_steps_in, predict_next_no_of_output)\n",
    "    \n",
    "# Train-test-val dataset split\n",
    "X_train1 = X[:int(no_of_train*len(X))]\n",
    "y_train1 = y[:int(no_of_train*len(X))]\n",
    "X_test   = X[int(no_of_train*len(X)):]\n",
    "y_test   = y[int(no_of_train*len(X)):]\n",
    "\n",
    "X_train = X_train1[:int(no_of_train*len(X_train1))]\n",
    "y_train = y_train1[:int(no_of_train*len(X_train1))]\n",
    "X_val   = X_train1[int(no_of_train*len(X_train1)):]\n",
    "y_val   = y_train1[int(no_of_train*len(X_train1)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape,X_test.shape, y_test.shape,X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #***********************************\n",
    "    # Index 00-10 is for LSTM Model\n",
    "    # Index 11-20 is for Bi-LSTM Model\n",
    "    # Index 21-30 is for CNN Model\n",
    "    # Index 31-40 is for CNN-LSTM Model\n",
    "    #***********************************\n",
    "\n",
    "    # Create model and summary\n",
    "    model = createModel(X_train, predict_next_no_of_output, index) \n",
    "    model.summary()\n",
    "    \n",
    "    # Create checkpoint for the training\n",
    "    # This checkpoint performs model saving when\n",
    "    # an epoch gives highest testing accuracy  \n",
    "    checkpoint     = ModelCheckpoint(filepath, monitor='val_loss',verbose=0,save_best_only=True,mode='min') \n",
    "    \n",
    "    # Log the epoch detail into csv\n",
    "    csv_logger     = CSVLogger(modelname + '.csv')\n",
    "    callbacks_list = [checkpoint,csv_logger]\n",
    "\n",
    "    # steps_per_epoch = total training data across all classes / batch size\n",
    "    # validation_steps = number of batches in validation dataset defining 1 epoch\n",
    "    # The line for training\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data=(X_val, y_val), \n",
    "                        epochs=no_of_epoch, \n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        callbacks=callbacks_list) \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved trained model and scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your own trained model & scaler\n",
    "model           = load_model(filepath, compile = False)\n",
    "scaler_filename = \"grab_msft_scaler.save\"\n",
    "scaler          = joblib.load(scaler_filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value = model.predict(X_train)\n",
    "print(np.sqrt(mean_squared_error(y_train,predicted_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_value = model.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(y_val,predicted_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_value = model.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test,predicted_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "records     = pd.read_csv(modelname +'.csv')\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.xticks(records['epoch'])\n",
    "\n",
    "plt.plot(records['mse'],label=\"mse\")\n",
    "plt.plot(records['val_mse'],label=\"val_mse\")\n",
    "plt.title('MSE-MAE',fontsize=12)\n",
    "plt.legend(loc=\"upper left\",fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls  = pd.ExcelFile('data/test_sorted_sample8000.xlsx')\n",
    "test_sample = pd.read_excel(xls, 'TrafficMgmt')\n",
    "\n",
    "# Apply Standardscalar to sample test data\n",
    "test_sample[predictor_var] = scaler.fit_transform(test_sample[predictor_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert into input/output\n",
    "test_sample_array = test_sample.to_numpy()\n",
    "X_sample, y_sample = split_sequences(test_sample_array, n_steps_in, predict_next_no_of_output)\n",
    "print(X_sample.shape, y_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value = model.predict(X_sample)\n",
    "print('Predicted value : {} \\n Actual Value    : {}' .format(predicted_value,y_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(mean_squared_error(y_sample,predicted_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
